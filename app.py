import os
import asyncio
import gradio as gr
import edge_tts
from src.utils.preprocess import CropAndExtract
from src.test_audio2coeff import Audio2Coeff
from src.facerender.animate import AnimateFromCoeff
from src.generate_batch import get_data
from src.generate_facerender_batch import get_facerender_data
from src.utils.init_path import init_path

# è¨­å®šæ¨¡å‹è·¯å¾‘èˆ‡åˆå§‹åŒ–
def TTS_generate(text, voice='zh-TW-HsiaoChenNeural'):
    """å°‡æ–‡å­—è½‰ç‚ºèªéŸ³æª”æ¡ˆ"""
    output_audio = "input_audio.mp3"
    communicate = edge_tts.Communicate(text, voice)
    asyncio.run(communicate.save(output_audio))
    return output_audio

def generate_anchor_video(source_image, text, voice):
    # 1. ç”¢ç”ŸéŸ³è¨Š
    audio_path = TTS_generate(text, voice)
    
    # 2. è¨­å®š SadTalker åƒæ•¸ (ç°¡åŒ–ç‰ˆé‚è¼¯)
    # æ³¨æ„ï¼šå¯¦éš›é‹è¡Œéœ€è¼‰å…¥ SadTalker çš„ç›¸é—œé¡åˆ¥èˆ‡æ¨¡å‹è·¯å¾‘
    # é€™è£¡å»ºè­°åƒè€ƒ SadTalker å®˜æ–¹çš„ inference.py é‚è¼¯
    checkpoint_path = './checkpoints'
    config_path = './src/config'
    
    # å‡è¨­èª¿ç”¨ SadTalker çš„æ ¸å¿ƒç”Ÿæˆå‡½æ•¸
    # å»ºè­°ç›´æ¥èª¿ç”¨å®˜æ–¹æä¾›çš„æ¨è«–å…¥å£ï¼Œä»¥ä¸‹ç‚ºé‚è¼¯ç¤ºæ„ï¼š
    print(f"æ­£åœ¨è™•ç†: {source_image} èˆ‡ {audio_path}")
    
    # ç”Ÿæˆçµæœè·¯å¾‘ (SadTalker é è¨­æœƒå­˜æ”¾åœ¨ results ç›®éŒ„)
    output_video_path = "results/output_video.mp4"
    
    # è¿”å›ç”Ÿæˆçš„å½±ç‰‡è·¯å¾‘çµ¦ Gradio
    return output_video_path

# å»ºç«‹ Gradio ä»‹é¢
with gr.Blocks() as demo:
    gr.Markdown("# ğŸ¤– AI æ•¸ä½ä¸»æ’­ç”Ÿæˆå™¨")
    
    with gr.Row():
        with gr.Column():
            input_img = gr.Image(type="filepath", label="ä¸Šå‚³äººåƒç…§ç‰‡")
            input_text = gr.Textbox(label="ä¸»æ’­è®€ç¨¿æ–‡å­—", lines=5, placeholder="è«‹è¼¸å…¥è¦æ’­å ±çš„å…§å®¹...")
            voice_opt = gr.Dropdown(
                choices=["zh-TW-HsiaoChenNeural", "zh-TW-YunJheNeural", "zh-CN-XiaoxiaoNeural"], 
                value="zh-TW-HsiaoChenNeural", 
                label="é¸æ“‡èªéŸ³ (å¾®è»Ÿ Edge-TTS)"
            )
            submit_btn = gr.Button("é–‹å§‹ç”Ÿæˆä¸»æ’­å½±ç‰‡", variant="primary")
        
        with gr.Column():
            output_video = gr.Video(label="ç”Ÿæˆçµæœ")

    submit_btn.click(
        fn=generate_anchor_video,
        inputs=[input_img, input_text, voice_opt],
        outputs=output_video
    )

demo.launch()
